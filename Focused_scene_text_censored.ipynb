{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "94s7R_UiMWqj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install boto3\n",
        "!pip install Levenshtein\n",
        "!pip install python-Levenshtein\n",
        "!pip install azure-cognitiveservices-vision-computervision boto3\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCodLq9uMwxB",
        "outputId": "7727c3f2-9f2b-4347-c670-5b98894c2fb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Xha2JuNHGX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import requests\n",
        "import boto3\n",
        "from difflib import SequenceMatcher\n",
        "import logging\n",
        "from openai import OpenAI\n",
        "import Levenshtein\n",
        "import pytesseract\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCEZ0-1SOyCI"
      },
      "outputs": [],
      "source": [
        "folder = \"/content/drive/MyDrive/vision_datasets/ICDAR FOCUSED SCENE TEXT\"\n",
        "\n",
        "#folder=\"/content/test_data/test\"\n",
        "#folder=r\"C:\\Users\\franc\\OneDrive\\Desktop\\vision project\\ICDAR FOCUSED SCENE TEXT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whHKnZ09OFyX"
      },
      "outputs": [],
      "source": [
        "max_size = 1 * 1024 * 1024\n",
        "\n",
        "#image_folder=os.path.join(folder,\"Images\")\n",
        "#text_folder=os.path.join(folder,\"Text\")\n",
        "\n",
        "images_paths = sorted(\n",
        "    [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith((\".jpg\", \".png\", \".jpeg\")) and os.path.getsize(os.path.join(folder, file)) < max_size]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pfd-fcCzFfg"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLmrBQ68zFfg",
        "outputId": "4d2d1325-e2d2-4e59-8372-068ed0ba978f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tiredness', 'kills', 'A', 'short', 'break', 'could', 'save', 'your', 'life']\n"
          ]
        }
      ],
      "source": [
        "# Funzione per estrarre parole da un file di testo\n",
        "\n",
        "\n",
        "def estrai_parole_da_file(percorso_file):\n",
        "    parole = []\n",
        "    with open(percorso_file, 'r') as file:\n",
        "        for riga in file:\n",
        "            # Extract the last word or quoted word after the last comma\n",
        "            match = re.search(r',\\s*\"?([^\",]+)\"?\\s*$', riga.strip())\n",
        "            if match:\n",
        "                transcription = match.group(1)\n",
        "                # Ignore transcriptions that are ###\n",
        "                if transcription != \"###\":\n",
        "                    parole.append(transcription)\n",
        "    return parole\n",
        "#ESEMPIO\n",
        "# Percorso al file di testo\n",
        "percorso_file = \"/content/drive/MyDrive/vision_datasets/ICDAR FOCUSED SCENE TEXT/gt_img_1.txt\"\n",
        "\n",
        "# Estrazione delle parole\n",
        "parole_estratte = estrai_parole_da_file(percorso_file)\n",
        "print(parole_estratte)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBYIjdfwJ8sU"
      },
      "source": [
        "# LLM Initializitation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LOFrmYJzNHI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aevdoGH7zNHI",
        "outputId": "c6787a70-ded8-41ec-b06e-cb0f0e209e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdH5m5kszNHJ"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlRtkSAQzNHM"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGgvAJqnzNHM"
      },
      "outputs": [],
      "source": [
        "prompt_template='''You are an advanced language model specialized in text correction. You will receive text extracted from images using OCR (Optical Character Recognition) models. Your task is to identify and correct any misspelled words or inaccuracies while preserving the context and intended meaning of the text.\n",
        "\n",
        "### Instructions:\n",
        "1. Correct spelling errors: Replace any misspelled words with the correct ones.\n",
        "2. Maintain context: Ensure that the corrected text aligns with the overall meaning and structure of the original input.\n",
        "3. Handle OCR-specific errors:\n",
        "   - Fix common OCR mistakes such as incorrect substitutions of similar-looking characters (e.g., \"rn\" misread as \"m\").\n",
        "   - Handle mixed-case errors, such as \"tHiS iS\" to \"This is.\"\n",
        "4. Do not alter proper nouns, numbers, or special characters unless they are obviously incorrect.\n",
        "5. Do not add or remove punctuations\n",
        "\n",
        "Output the corrected text clearly and concisely.\n",
        "\n",
        "### Example Inputs and Outputs:\n",
        "\n",
        "**Input:**\n",
        "Th1s 1s an exarnple of OCR t3xt.\n",
        "\n",
        "**Output:**\n",
        "This is an example of OCR text.\n",
        "\n",
        "**Input:**\n",
        "Th@ rn0del w1ll c0rrect err0rs.\n",
        "\n",
        "**Output:**\n",
        "The model will correct errors.\n",
        "\n",
        "Focus on accuracy and consistency. Your goal is to produce clean and coherent text that closely resembles the intended content. Only output the corrected text and noting else'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2bRn2cUzNHN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            prompt_template,\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob2EtKyJzNHN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.string import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWL2WvYb_yo_"
      },
      "source": [
        "Esempio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ytWVs35AzNHN",
        "outputId": "262e6e45-e1ec-47da-e37b-4eafe29605e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello, my name is John'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=prompt|llm|StrOutputParser()\n",
        "\n",
        "model.invoke({'input':\"Hrllo,my name is john\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0D47WKCUTx6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Functions for Metrics\n",
        "def calculate_levenshtein_accuracy(extracted_text, ground_truth):\n",
        "    extracted_text = extracted_text.strip().lower()\n",
        "    ground_truth = ground_truth.strip().lower()\n",
        "    distance = Levenshtein.distance(extracted_text, ground_truth)\n",
        "    max_length = max(len(extracted_text), len(ground_truth))\n",
        "    return (1 - distance / max_length) * 100 if max_length > 0 else 0\n",
        "\n",
        "def calculate_word_accuracy(expected_words, ocr_words):\n",
        "    expected_set = set(word.lower() for word in expected_words)\n",
        "    ocr_set = set(word.lower() for word in ocr_words)\n",
        "    matching_words = expected_set & ocr_set\n",
        "    return (len(matching_words) / len(expected_set)) * 100 if expected_set else 0\n",
        "\n",
        "def calculate_character_error_rate(extracted_text, ground_truth):\n",
        "    errors = Levenshtein.distance(extracted_text, ground_truth)\n",
        "    total_chars = len(ground_truth)\n",
        "    return (errors / total_chars) * 100 if total_chars > 0 else 0\n",
        "\n",
        "# Evaluation Pipeline\n",
        "def evaluate_ocr(ocr_dict,type):\n",
        "    \"\"\"\n",
        "    Evaluate OCR output against ground truth for each entry in a dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        ocr_dict (dict): Dictionary with keys as image names and values as OCR outputs.\n",
        "        type: 'scene' o 'document'\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with image names as keys and evaluation metrics as values.\n",
        "    \"\"\"\n",
        "    evaluation_results = {}\n",
        "    #image_folder_path = os.path.join(folder, \"Images\")\n",
        "    #text_folder_path = os.path.join(folder, \"Text\")\n",
        "\n",
        "    for image_name, ocr_output in ocr_dict.items():\n",
        "\n",
        "        ground_truth_file = f\"{folder}/{os.path.splitext(image_name)[0]}.json\"\n",
        "        if type==\"scene\":\n",
        "            ground_truth_file=os.path.join(folder,'gt_'+ image_name.rsplit('.', 1)[0] + '.txt')\n",
        "        if type==\"document\":\n",
        "            ground_truth_file=os.path.join(folder, image_name.rsplit('.', 1)[0] + '.txt')\n",
        "\n",
        "      #output_json_path = os.path.join(folder, image_name.rsplit('.', 1)[0] + '_ocr.json')\n",
        "\n",
        "\n",
        "\n",
        "        # Assuming ground truth JSON has a key \"text\" containing the expected output\n",
        "        #ground_truth = \" \".join(item[\"text\"] for item in ground_truth_data if \"text\" in item)\n",
        "\n",
        "        ground_truth_words = estrai_parole_da_file(ground_truth_file)\n",
        "        ground_truth_text=\" \".join(item for item in ground_truth_words)\n",
        "        lev_acc_baseline = calculate_levenshtein_accuracy(ocr_output, ground_truth_text)\n",
        "        word_acc_baseline = calculate_word_accuracy(ground_truth_words, ocr_output.split())\n",
        "        cer_baseline = calculate_character_error_rate(ocr_output, ground_truth_text)\n",
        "\n",
        "        #LLM\n",
        "        llm_output=model.invoke({'input':ocr_output})\n",
        "        time.sleep(5)#Con il piano gratuito, possiamo fare solo 15 run al minuto\n",
        "        lev_acc_llm = calculate_levenshtein_accuracy(llm_output, ground_truth_text)\n",
        "        word_acc_llm = calculate_word_accuracy(ground_truth_words, llm_output.split())\n",
        "        cer_llm = calculate_character_error_rate(llm_output, ground_truth_text)\n",
        "\n",
        "\n",
        "\n",
        "        # Store the results in the evaluation dictionary\n",
        "        evaluation_results[os.path.splitext(image_name)[0]] = {\n",
        "            \"Levenshtein Accuracy\": lev_acc_baseline,\n",
        "            \"Word Accuracy\": word_acc_baseline,\n",
        "            \"Character Error Rate\": cer_baseline,\n",
        "            \"Levenshtein Accuracy LLM\": lev_acc_llm,\n",
        "            \"Word Accuracy LLM\": word_acc_llm,\n",
        "            \"Character Error Rate LLM\": cer_llm\n",
        "\n",
        "        }\n",
        "    return evaluation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGLksNnNt7W"
      },
      "source": [
        "## AZURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZLm-J41NaBN"
      },
      "outputs": [],
      "source": [
        "subscription_key = AZURE_API_KEY\n",
        "endpoint = \"https://patrikbaldon.cognitiveservices.azure.com/\"\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvtxnUBOPdA"
      },
      "outputs": [],
      "source": [
        "azure_extraction = {}\n",
        "\n",
        "# Funzione per estrarre testo e aggiungerlo al dizionario\n",
        "def extract_text_azure(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        read_response = computervision_client.read_in_stream(image_file, raw=True)\n",
        "        operation_location = read_response.headers[\"Operation-Location\"]\n",
        "        operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "        # Attendi il completamento\n",
        "        while True:\n",
        "            result = computervision_client.get_read_result(operation_id)\n",
        "            if result.status not in ['notStarted', 'running']:\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Estrai il testo\n",
        "        extracted_text = \"\"\n",
        "        if result.status == OperationStatusCodes.succeeded:\n",
        "            for page in result.analyze_result.read_results:\n",
        "                for line in page.lines:\n",
        "                    extracted_text += line.text + \" \"\n",
        "\n",
        "        # Salva nel dizionario\n",
        "        azure_extraction[os.path.basename(image_path)] = extracted_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TTJ0vyi1PFE-"
      },
      "outputs": [],
      "source": [
        "# Processa ogni immagine\n",
        "for image_path in images_paths[0:150]:\n",
        "    extract_text_azure(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dPepBsOi7R6"
      },
      "outputs": [],
      "source": [
        "Azure_Evaluation = evaluate_ocr(azure_extraction,'scene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypeRx5koatqD",
        "outputId": "3b1998c5-53ab-45dd-d895-9406d9e5c41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First record:\n",
            "Key: img_1\n",
            "Value: {'Levenshtein Accuracy': 100.0, 'Word Accuracy': 100.0, 'Character Error Rate': 0.0, 'Levenshtein Accuracy LLM': 96.15384615384616, 'Word Accuracy LLM': 77.77777777777779, 'Character Error Rate LLM': 4.0}\n"
          ]
        }
      ],
      "source": [
        "keys = list(Azure_Evaluation.keys())\n",
        "values = list(Azure_Evaluation.values())\n",
        "\n",
        "print(f\"First record:\\nKey: {keys[0]}\\nValue: {values[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2AbV54Nwxa"
      },
      "source": [
        "# AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "497UcGUKNhvR"
      },
      "outputs": [],
      "source": [
        "# Configura manualmente le credenziali e la regione\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=AWS_API_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET,\n",
        "    region_name='us-east-1'\n",
        ")\n",
        "bucket_name = \"patriksbucket\"\n",
        "# Esempio: utilizza il client Textract\n",
        "# Inizializza i client con la sessione\n",
        "s3 = session.client('s3')\n",
        "textract = session.client('textract')\n",
        "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=\"ICDAR FOCUSED SCENE TEXT/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlp6N15cTmPF"
      },
      "outputs": [],
      "source": [
        "file_names = [os.path.splitext(os.path.basename(path))[0] for path in images_paths]\n",
        "print(file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPcVbUZiV1No"
      },
      "outputs": [],
      "source": [
        "def list_images_in_folder(bucket):\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=AWS_API_KEY,\n",
        "        aws_secret_access_key=AWS_SECRET,\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "    s3 = session.client('s3')\n",
        "    response = s3.list_objects_v2(Bucket=bucket, Prefix=\"ICDAR FOCUSED SCENE TEXT/\")\n",
        "\n",
        "    # Dimensione massima in bytes (1 MB)\n",
        "    max_size = 1 * 1024 * 1024  # 1 MB\n",
        "\n",
        "    # Ottieni e ordina le immagini per nome\n",
        "    sorted_objects = sorted(\n",
        "        [obj for obj in response.get('Contents', []) if obj['Key'].endswith(('.jpg', '.png', '.jpeg'))],\n",
        "        key=lambda x: x['Key']\n",
        "    )\n",
        "\n",
        "    # Filtra immagini per dimensione\n",
        "    return [\n",
        "        (obj['Key'], obj['Key'].split('/')[-1].split('.')[0])\n",
        "        for obj in sorted_objects\n",
        "        if (obj['Size'] < max_size and obj['Key'].split('/')[-1].split('.')[0] in file_names)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWp7AEH6VSC5"
      },
      "outputs": [],
      "source": [
        "def extract_text_textract(bucket, image_name):\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=AWS_API_KEY,\n",
        "        aws_secret_access_key=AWS_SECRET,\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "    textract = session.client('textract')\n",
        "    response = textract.detect_document_text(\n",
        "        Document={'S3Object': {'Bucket': bucket, 'Name': image_name}}\n",
        "    )\n",
        "    extracted_text = []\n",
        "    for block in response['Blocks']:\n",
        "        if block['BlockType'] == 'LINE':\n",
        "            extracted_text.append(block['Text'])\n",
        "    return \" \".join(extracted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UG34BkGWpMS"
      },
      "outputs": [],
      "source": [
        "# Configurazione AWS\n",
        "region_name = \"us-east-1\"\n",
        "bucket_name = \"patriksbucket\"\n",
        "\n",
        "aws_extraction = {}\n",
        "\n",
        "\n",
        "files = list_images_in_folder(bucket_name)\n",
        "\n",
        "\n",
        "for path, image_name in files[0:150]:\n",
        "  # Estrai testo dall'immagine\n",
        "  aws_extraction[image_name] = extract_text_textract(bucket_name, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqDJYC_aZ3pQ"
      },
      "outputs": [],
      "source": [
        "#folder = \"/content/drive/MyDrive/vision_datasets/Handwriting\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oH-GzRlj-oN"
      },
      "outputs": [],
      "source": [
        "Aws_Evaluation = evaluate_ocr(aws_extraction,'scene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPcOwyFvkaW2",
        "outputId": "1d4eaaa8-f5cd-461e-d563-76d1ef760864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First record:\n",
            "Key: img_1\n",
            "Value: {'Levenshtein Accuracy': 94.0, 'Word Accuracy': 66.66666666666666, 'Character Error Rate': 6.0, 'Levenshtein Accuracy LLM': 87.27272727272728, 'Word Accuracy LLM': 66.66666666666666, 'Character Error Rate LLM': 14.000000000000002}\n"
          ]
        }
      ],
      "source": [
        "keys = list(Aws_Evaluation.keys())\n",
        "values = list(Aws_Evaluation.values())\n",
        "\n",
        "print(f\"First record:\\nKey: {keys[0]}\\nValue: {values[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMUBiaD2N0Mx"
      },
      "source": [
        "# OCR_SPACE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYA5TCa1Z8AK"
      },
      "outputs": [],
      "source": [
        "#folder=\"/content/test_data/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEa3C9cXNr1f"
      },
      "outputs": [],
      "source": [
        "API_KEY = OCR_SPACE_API_KEY\n",
        "ocrspace_extraction = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDrWs-bsktlb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ocr_space_file(image_path, output_path=None, overlay=False, api_key=None, language='eng'):\n",
        "    \"\"\"\n",
        "    OCR.space API request with a local file, optionally saving results as JSON.\n",
        "\n",
        "    Parameters:\n",
        "        image_path (str): Path to the file.\n",
        "        output_path (str, optional): Path to save the resulting JSON file.\n",
        "        overlay (bool, optional): Include OCR overlay in the response.\n",
        "        api_key (str): Your OCR.space API key.\n",
        "        language (str): Language code for OCR.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing image names as keys and extracted text as values.\n",
        "    \"\"\"\n",
        "\n",
        "    payload = {\n",
        "        'isOverlayRequired': overlay,\n",
        "        'apikey': api_key,\n",
        "        'language': language,\n",
        "        'OCREngine': 2\n",
        "    }\n",
        "\n",
        "\n",
        "    # Open the image file in binary mode\n",
        "    with open(image_path, 'rb') as f:\n",
        "      r = requests.post(\n",
        "      'https://api.ocr.space/parse/image',\n",
        "      files={os.path.basename(image_path): f},\n",
        "      data=payload,\n",
        "      )\n",
        "      result = r.json()\n",
        "\n",
        "      # Skip if there are errors in the response\n",
        "      #if result.get(\"IsErroredOnProcessing\"):\n",
        "      #  return extracted_text\n",
        "\n",
        "      ## Extract the text and remove newlines\n",
        "      extracted_text = result[\"ParsedResults\"][0][\"ParsedText\"].replace('\\n', ' ')\n",
        "    return extracted_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aUujbvgHq-q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def ocr_space_file(image_path, output_path=None, overlay=False, api_key=None, language='eng'):\n",
        "    \"\"\"\n",
        "    OCR.space API request with a local file, optionally saving results as JSON.\n",
        "\n",
        "    Parameters:\n",
        "        image_path (str): Path to the file.\n",
        "        output_path (str, optional): Path to save the resulting JSON file.\n",
        "        overlay (bool, optional): Include OCR overlay in the response.\n",
        "        api_key (str): Your OCR.space API key.\n",
        "        language (str): Language code for OCR.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the image or None if an error occurred.\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        'isOverlayRequired': overlay,\n",
        "        'apikey': api_key,\n",
        "        'language': language,\n",
        "        'OCREngine': 2\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Open the image file in binary mode\n",
        "        with open(image_path, 'rb') as f:\n",
        "            r = requests.post(\n",
        "                'https://api.ocr.space/parse/image',\n",
        "                files={os.path.basename(image_path): f},\n",
        "                data=payload,\n",
        "            )\n",
        "        result = r.json()\n",
        "\n",
        "        # Check if the response contains errors\n",
        "        #if result.get(\"IsErroredOnProcessing\", False):\n",
        "        #    print(f\"Error processing {image_path}: {result.get('ErrorMessage', 'Unknown error')}\")\n",
        "        #    return None\n",
        "\n",
        "        # Extract the text\n",
        "        return result[\"ParsedResults\"][0][\"ParsedText\"].replace('\\n', ' ')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred for {image_path}: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh_ULJSfH0Np"
      },
      "outputs": [],
      "source": [
        "# Process multiple images\n",
        "ocrspace_extraction = {}\n",
        "for image_path in images_paths[0:150]:\n",
        "    text = ocr_space_file(image_path, api_key=OCR_SPACE_API_KEY)\n",
        "    time.sleep(1)\n",
        "    if text:  # Only add if text extraction succeeded\n",
        "        ocrspace_extraction[os.path.basename(image_path)] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oxsqmSsFuKj",
        "outputId": "dfecd81f-3cae-463c-c3e2-7057ea1afd74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'img_1.jpg': 'Tiredness kills A short break could save your life',\n",
              " 'img_100.jpg': 'JOHN RENENT (Buildeis GELCON www.celcon.co.uk',\n",
              " 'img_101.jpg': 'No cycling',\n",
              " 'img_102.jpg': 'COLTA MED mediterranean cuisine',\n",
              " 'img_103.jpg': 'FIRED EARTH',\n",
              " 'img_104.jpg': 'lowns-uk.co',\n",
              " 'img_105.jpg': 'HOME ENTERTAINMENT',\n",
              " 'img_106.jpg': 'HFC SANE',\n",
              " 'img_107.jpg': 'STAR WARS EPISODE II: ATTACK OF THE CLONES',\n",
              " 'img_108.jpg': 'STAR WARS',\n",
              " 'img_109.jpg': 'SPIDER MAN',\n",
              " 'img_11.jpg': \"M. Summer's Here!\",\n",
              " 'img_110.jpg': 'FRANSTORMERS',\n",
              " 'img_111.jpg': 'NATURAL HISTORY MUSEUM',\n",
              " 'img_112.jpg': 'CLOSING DOWN SALE',\n",
              " 'img_113.jpg': 'one hour the eyecare clinic contact lens centre',\n",
              " 'img_114.jpg': 'act one bar...',\n",
              " 'img_115.jpg': 'FUN FASHION LADIES LINGERIE SHOP 01206 368166 FUNFASHION@HOTMAIL.COM',\n",
              " 'img_116.jpg': 'P',\n",
              " 'img_117.jpg': 'Counselling Helpline Lifelines Family Support',\n",
              " 'img_118.jpg': 'UK DANCE PROTOTYPE & INSPIRED RECORDS',\n",
              " 'img_119.jpg': 'First fo Eastern National Bus Times',\n",
              " 'img_120.jpg': 'Next Departure 12:31 2C FIRST E',\n",
              " 'img_121.jpg': 'KNOW YOUR First f TICKET!',\n",
              " 'img_122.jpg': 'Ipswich London ( A 12) Town Centre Longridge Park 300',\n",
              " 'img_123.jpg': 'PREMIER TEAM',\n",
              " 'img_124.jpg': 'Famous for our fish & chips',\n",
              " 'img_125.jpg': 'BUFFETT WAY CUL - DE - SAC',\n",
              " 'img_126.jpg': 'University of Essex 1 © leading to Wivenhoe Trail Greenstead centre 2 14',\n",
              " 'img_127.jpg': 'DANGER KEEP OUT',\n",
              " 'img_128.jpg': 'EMOP. 0.0%5',\n",
              " 'img_129.jpg': '',\n",
              " 'img_13.jpg': 'IR',\n",
              " 'img_130.jpg': 'University of Essex leading to Wivenhoe Trail Greenstead centre 1',\n",
              " 'img_131.jpg': 'Shh',\n",
              " 'img_132.jpg': 'University of Essex Day Nursery The Houses Keynes, Rayleigh, Tawney and William Morris Towers Wolfson Court Buses and Cycles Only CONFERENCE CAR PARK Entrance 4'}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ocrspace_extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf_PTNNtFgNN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgCt_m2SqFAM"
      },
      "outputs": [],
      "source": [
        "OcrSpace_Evaluation = evaluate_ocr(ocrspace_extraction , 'scene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBc22SXLqKRJ",
        "outputId": "663fa0ce-f376-44bb-9f18-01340da3c2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First record:\n",
            "Key: img_1\n",
            "Value: {'Levenshtein Accuracy': 100.0, 'Word Accuracy': 100.0, 'Character Error Rate': 0.0, 'Levenshtein Accuracy LLM': 96.15384615384616, 'Word Accuracy LLM': 77.77777777777779, 'Character Error Rate LLM': 4.0}\n"
          ]
        }
      ],
      "source": [
        "keys = list(OcrSpace_Evaluation.keys())\n",
        "values = list(OcrSpace_Evaluation.values())\n",
        "\n",
        "print(f\"First record:\\nKey: {keys[0]}\\nValue: {values[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YUyXnslzFft"
      },
      "source": [
        "esempio output ocr space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S15wrfNzFft",
        "outputId": "35e597c0-851c-458a-e8c9-9955929262fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'img_109.jpg': 'SPIDER MAN',\n",
              " 'img_11.jpg': \"iM. Summer's Here!\",\n",
              " 'img_110.jpg': '123 FANSTORMERS GRNGD2',\n",
              " 'img_111.jpg': 'NATURAL HISTORY MUSEUM',\n",
              " 'img_112.jpg': 'CLOSING DOWN SALE',\n",
              " 'img_113.jpg': 'one hour the eyecare clinic contact lens centre',\n",
              " 'img_114.jpg': 'act one',\n",
              " 'img_115.jpg': 'FUN FASHION LADIES LINGERIE SHOP 01206 368166 FUNFASHION@HOTMAIL.COM',\n",
              " 'img_116.jpg': 'P',\n",
              " 'img_117.jpg': 'Counselling Helpline Lifelines Family Support'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ocrspace_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzXo_CxA6OkB"
      },
      "source": [
        "# TESSERACT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi2gu_Ll68cg"
      },
      "outputs": [],
      "source": [
        "tesseract_extraction = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyOMTr6L6Sal"
      },
      "outputs": [],
      "source": [
        "# Function to extract text using Tesseract OCR\n",
        "def extract_text_tesseract(image_path):\n",
        "  text = pytesseract.image_to_string(Image.open(image_path))\n",
        "  return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Fy4KKR7GDA"
      },
      "outputs": [],
      "source": [
        "for image_path in images_paths[0:150]:\n",
        "  tesseract_extraction[os.path.basename(image_path)] = extract_text_tesseract(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwZ6hXVB8IwX"
      },
      "outputs": [],
      "source": [
        "Tesseract_Evaluation = evaluate_ocr(tesseract_extraction ,'scene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFjqRPz08SV8",
        "outputId": "fab960c9-eb06-451f-eb34-93cdaabc192a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First record:\n",
            "Key: img_1\n",
            "Value: {'Levenshtein Accuracy': 36.0, 'Word Accuracy': 11.11111111111111, 'Character Error Rate': 68.0, 'Levenshtein Accuracy LLM': 40.0, 'Word Accuracy LLM': 22.22222222222222, 'Character Error Rate LLM': 62.0}\n"
          ]
        }
      ],
      "source": [
        "keys = list(Tesseract_Evaluation.keys())\n",
        "values = list(Tesseract_Evaluation.values())\n",
        "\n",
        "print(f\"First record:\\nKey: {keys[0]}\\nValue: {values[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2oqlZ2H2nt6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8P95qxkevKc"
      },
      "source": [
        "# GOOGLE CLOUD VISION API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "-MDuRRfqS7ug",
        "outputId": "30c7aa90-f19b-41fa-978f-0af6aecaacce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (4.25.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.12.14)\n",
            "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/514.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "63b8e1b241bc40c7a529cba6c4d58e0e",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YRrzttyequu"
      },
      "outputs": [],
      "source": [
        "cloudvisionapikey = CLOUD_VISION_API_KEY\n",
        "cloudvision_extraction = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwhPI5Zve6uz"
      },
      "outputs": [],
      "source": [
        "from google.cloud import vision\n",
        "from google.auth.transport.requests import Request\n",
        "from google.auth.credentials import AnonymousCredentials\n",
        "import requests\n",
        "import base64 # Import base64\n",
        "\n",
        "def detect_text(image_path):\n",
        "    \"\"\"\n",
        "    Detect text in an image using the Vision API with an API key.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Your Google Cloud Vision API key.\n",
        "        image_path (str): Path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        str: Detected text.\n",
        "    \"\"\"\n",
        "    # Read the image file\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        image_content = image_file.read()\n",
        "\n",
        "    # Encode image content to base64\n",
        "    image_content_base64 = base64.b64encode(image_content).decode('utf-8')\n",
        "\n",
        "    # Create the request payload\n",
        "    url = f\"https://vision.googleapis.com/v1/images:annotate?key={cloudvisionapikey}\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    payload = {\n",
        "        \"requests\": [\n",
        "            {\n",
        "                \"image\": {\"content\": image_content_base64}, # Use base64 encoded content\n",
        "                \"features\": [{\"type\": \"TEXT_DETECTION\"}]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Send the request\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse the response\n",
        "    result = response.json()\n",
        "    if \"responses\" in result and \"textAnnotations\" in result[\"responses\"][0]:\n",
        "        detected_text = result[\"responses\"][0][\"textAnnotations\"][0][\"description\"]\n",
        "        return detected_text\n",
        "    else:\n",
        "        return \"No text detected.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz4pMZipfXFj"
      },
      "outputs": [],
      "source": [
        "for image_path in images_paths[0:150]:\n",
        "  cloudvision_extraction[os.path.basename(image_path)] = detect_text(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMBGxMHgfak0"
      },
      "outputs": [],
      "source": [
        "OcrCloudVision_Evaluation = evaluate_ocr(cloudvision_extraction,'scene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nngHxoSDfdUp",
        "outputId": "e52bdca5-7eb7-4fa5-dbe9-dc3230111330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First record:\n",
            "Key: img_1\n",
            "Value: {'Levenshtein Accuracy': 92.0, 'Word Accuracy': 100.0, 'Character Error Rate': 8.0, 'Levenshtein Accuracy LLM': 92.0, 'Word Accuracy LLM': 100.0, 'Character Error Rate LLM': 8.0}\n"
          ]
        }
      ],
      "source": [
        "keys = list(OcrCloudVision_Evaluation.keys())\n",
        "values = list(OcrCloudVision_Evaluation.values())\n",
        "\n",
        "print(f\"First record:\\nKey: {keys[0]}\\nValue: {values[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlihV30s3M95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND9aNNba3OFK"
      },
      "source": [
        "# COMPARE THE EVALUATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-_liZJw7mhV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1maFZ573OFM"
      },
      "outputs": [],
      "source": [
        "def calculate_score(metrics, alpha, beta, gamma):\n",
        "\n",
        "    lev_accuracy = metrics.get(\"Levenshtein Accuracy\", 0)\n",
        "    word_accuracy = metrics.get(\"Word Accuracy\", 0)\n",
        "    cer = metrics.get(\"Character Error Rate\", 0)\n",
        "\n",
        "    total_score = alpha * lev_accuracy + beta * word_accuracy - gamma * cer\n",
        "    return total_score\n",
        "\n",
        "def calculate_score_llm(metrics, alpha, beta, gamma):\n",
        "\n",
        "    lev_accuracy = metrics.get(\"Levenshtein Accuracy LLM\", 0)\n",
        "    word_accuracy = metrics.get(\"Word Accuracy LLM\", 0)\n",
        "    cer = metrics.get(\"Character Error Rate LLM\", 0)\n",
        "\n",
        "    total_score = alpha * lev_accuracy + beta * word_accuracy - gamma * cer\n",
        "    return total_score\n",
        "\n",
        "\n",
        "def evaluate_all_metrics(evaluations, alpha, beta, gamma):\n",
        "\n",
        "    best_api = None\n",
        "    best_score = float('-inf')\n",
        "    results=[]\n",
        "\n",
        "    print(\"API Scores:\\n\")\n",
        "\n",
        "    for api_name, image_metrics in evaluations.items():\n",
        "        total_score = 0\n",
        "        total_score_llm = 0\n",
        "        total_lev_accuracy = 0\n",
        "        total_word_accuracy = 0\n",
        "        total_cer = 0\n",
        "        total_lev_accuracy_llm = 0\n",
        "        total_word_accuracy_llm = 0\n",
        "        total_cer_llm = 0\n",
        "        num_images = len(image_metrics)\n",
        "        for image_name, metrics in image_metrics.items():\n",
        "            total_score += calculate_score(metrics, alpha, beta, gamma)\n",
        "            total_score_llm += calculate_score_llm(metrics, alpha, beta, gamma)\n",
        "\n",
        "            total_lev_accuracy += metrics.get(\"Levenshtein Accuracy\", 0)\n",
        "            total_word_accuracy += metrics.get(\"Word Accuracy\", 0)\n",
        "            total_cer += metrics.get(\"Character Error Rate\", 0)\n",
        "\n",
        "            total_lev_accuracy_llm += metrics.get(\"Levenshtein Accuracy LLM\", 0)\n",
        "            total_word_accuracy_llm += metrics.get(\"Word Accuracy LLM\", 0)\n",
        "            total_cer_llm += metrics.get(\"Character Error Rate LLM\", 0)\n",
        "        avg_score = total_score / num_images if num_images > 0 else 0\n",
        "        avg_score_llm = total_score_llm / num_images if num_images > 0 else 0\n",
        "\n",
        "        avg_lev_accuracy = total_lev_accuracy / num_images if num_images > 0 else 0\n",
        "        avg_word_accuracy = total_word_accuracy / num_images if num_images > 0 else 0\n",
        "        avg_cer = total_cer / num_images if num_images > 0 else 0\n",
        "\n",
        "        avg_lev_accuracy_llm = total_lev_accuracy_llm / num_images if num_images > 0 else 0\n",
        "        avg_word_accuracy_llm = total_word_accuracy_llm / num_images if num_images > 0 else 0\n",
        "        avg_cer_llm = total_cer_llm / num_images if num_images > 0 else 0\n",
        "\n",
        "        print(f\"{api_name} - Average Score: {avg_score:.2f}, with LLM: {avg_score_llm:.2f}\")\n",
        "\n",
        "        results.append({\n",
        "            \"API Name\": api_name,\n",
        "            \"Levenshtein Accuracy\": avg_lev_accuracy,\n",
        "            \"Levenshtein Accuracy with LLM\": avg_lev_accuracy_llm,\n",
        "            \"Word Accuracy\": avg_word_accuracy,\n",
        "            \"Word Accuracy with LLM\": avg_word_accuracy_llm,\n",
        "            \"Character Error Rate\": avg_cer,\n",
        "            \"Character Error Rate with LLM\": avg_cer_llm,\n",
        "            \"Average Score\": avg_score,\n",
        "            \"Average Score with LLM\": avg_score_llm\n",
        "        })\n",
        "\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_api = api_name\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_excel(\"eng_focused_scene_ocr_evaluation_results.xlsx\", index=False)\n",
        "\n",
        "    print(\"\\nResults saved to excel file\")\n",
        "    return best_api, best_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLP1HVnT3OFM",
        "outputId": "53eb26c7-2535-4791-871c-b48e56149025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Scores:\n",
            "\n",
            "Azure OCR - Average Score: 132.89, with LLM: 62.43\n",
            "AWS Textract - Average Score: 99.95, with LLM: 27.47\n",
            "OCR.space - Average Score: 127.91, with LLM: 111.54\n",
            "Tesseract - Average Score: -103.92, with LLM: -318.72\n",
            "Google Cloud Vision API - Average Score: 93.35, with LLM: 84.34\n",
            "\n",
            "Results saved to excel file\n",
            "\n",
            "The best OCR API is Azure OCR with an average score of 132.89.\n"
          ]
        }
      ],
      "source": [
        "# Esempio di dati delle metriche\n",
        "evaluations = {\n",
        "    \"Azure OCR\": Azure_Evaluation,\n",
        "    \"AWS Textract\": Aws_Evaluation,\n",
        "    \"OCR.space\": OcrSpace_Evaluation,\n",
        "    \"Tesseract\": Tesseract_Evaluation,\n",
        "    \"Google Cloud Vision API\": OcrCloudVision_Evaluation\n",
        "}\n",
        "\n",
        "# Confronta le metriche per tutte le API\n",
        "best_api, best_score = evaluate_all_metrics(evaluations, alpha=1.3, beta=0.6, gamma=1.5)\n",
        "print(f\"\\nThe best OCR API is {best_api} with an average score of {best_score:.2f}.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
